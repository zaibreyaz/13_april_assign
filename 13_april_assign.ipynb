{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5521c8b-f99c-4de1-9a4c-ebe291ecc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1. What is Random Forest Regressor?\n",
    "\n",
    "    Ans: Random Forest Regressor is an ensemble learning algorithm that combines multiple decision trees to perform regression tasks. It employs the random forest technique \n",
    "         by creating a collection of decision trees, each trained on different subsets of data, and averages their predictions to provide an overall regression prediction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f0420-0b9e-4393-8447-2a28d1c9b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "\n",
    "    Ans: Random Forest Regressor reduces the risk of overfitting through two main techniques: random feature selection at each split, which reduces correlation and \n",
    "         promotes diversity among trees, and averaging the predictions of multiple trees, which helps to mitigate the impact of individual noisy or overfitted trees.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cae5eb-e4d0-46ed-bd0c-6338577f2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "\n",
    "    Ans: Random Forest Regressor aggregates the predictions of multiple decision trees by averaging their individual predictions. Each tree independently makes a prediction, \n",
    "         and the final prediction is obtained by averaging the predictions of all the trees, resulting in a more robust and accurate regression estimate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d95b57-c956-457f-9013-678016c9a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "\n",
    "   Ans: n_estimators: The number of decision trees in the random forest.\n",
    "        max_depth: The maximum depth of each decision tree.\n",
    "        min_samples_split: The minimum number of samples required to split an internal node.\n",
    "        min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "        max_features: The number of features to consider when looking for the best split.\n",
    "        bootstrap: Whether to use bootstrap samples or not.\n",
    "        random_state: The seed used by the random number generator for reproducibility.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b407f-2e44-4e31-8ac9-4c41b44727ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "\n",
    "    Ans: Random Forest Regressor and Decision Tree Regressor are both machine learning algorithms for regression tasks. However, Random Forest Regressor is an ensemble method \n",
    "         that combines multiple decision trees, whereas Decision Tree Regressor uses a single decision tree. Random Forest Regressor reduces overfitting, improves generalization,\n",
    "         and provides more robust predictions by aggregating the predictions of multiple trees. Decision Tree Regressor may be prone to overfitting and lacks the diversity and \n",
    "         robustness offered by random forests.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e410b-b186-402a-b315-ec4315779ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "\n",
    "    Ans: Advantages of Random Forest Regressor include robustness to outliers, reduction of overfitting, and good performance with default hyperparameters. \n",
    "         Disadvantages include potential for increased computational complexity and less interpretability compared to a single decision tree. It may also struggle with \n",
    "         extrapolation beyond the training data range.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee343923-657b-4316-8ff8-a5d02bcbc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q7. What is the output of Random Forest Regressor?\n",
    "\n",
    "    Ans: The output of a Random Forest Regressor is a continuous numerical value, representing the predicted regression target variable. The model combines predictions from \n",
    "         multiple decision trees within the ensemble to provide a final regression estimate for a given input.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41099011-3dd3-49e9-bc3d-08d9eb1db52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q8. Can Random Forest Regressor be used for classification tasks?\n",
    "\n",
    "    Ans: Yes, Random Forest Regressor can be used for classification tasks by using the \"Random Forest Classifier\" variant. It applies the random forest ensemble technique to \n",
    "         multiple decision trees to perform classification, where the output is a class label instead of a continuous numerical value.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
